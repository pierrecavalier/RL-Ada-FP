{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28765105-00f2-4b45-8699-6b215815898a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "actions = [0,1,2,3]\n",
    "action_coordinates = {0: np.array([1,0]),# down\n",
    "                      1: np.array([0,1]), # right\n",
    "                      2: np.array([-1,0]), # up\n",
    "                      3: np.array([0,-1])} # left\n",
    "gamma = .9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d3673e-cc10-4a8c-a9ec-8047d9addfaf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "On considère un environnement où on se déplace sur une grille de taille 4 x 12. Un état est un couple de la forme $(i,j)$ où $0\\leqslant i\\leqslant 3$ et $0\\leqslant j\\leqslant 11$. Comme pour le labyrinthe, l'ensemble des actions est $\\left\\{ 0,1,2,3 \\right\\}$ qui correspondent respectivement à bas, droite, haut, gauche. Pour certains états et certaines actions, l'état suivant obtenu n'est pas celui auquel on s'attend. Le code suivant définit une class `env` qui simule les interactions avec l'environnement. On peut interagir avec l'environnement en utilisant une instance de cette classe, avec un code comme le suivant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7bdd3-f67d-4c87-9e3b-777b1d270992",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class env:\n",
    "    height = 4\n",
    "    width = 12\n",
    "    s = (0,0)\n",
    "\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "    def take_action(self, a):\n",
    "        new_s = tuple(np.array(self.s)+action_coordinates[a])\n",
    "\n",
    "        if new_s[0]<0 or new_s[1]<0 or new_s[0] >= self.height or new_s[1] >= self.width:\n",
    "            r = 0\n",
    "        elif new_s == (0,self.width-1):\n",
    "            r = 1\n",
    "            self.s = (0,0)\n",
    "        elif new_s == (0,0):\n",
    "            r = 0\n",
    "        elif new_s[0]==0:\n",
    "            r = -10 \n",
    "            self.s = (0,0)\n",
    "        else:\n",
    "            r = 0\n",
    "            self.s = new_s\n",
    "\n",
    "        return r, self.s\n",
    "\n",
    "environment = env() # initialisé à l'état (0,0)\n",
    "r, s = env.take_action(1) # on choisit l'action 1 (droite), cela renvoie le gain r et le nouvel état s\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959f8c88-ad3a-4950-8289-1edb05123115",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Le code suivant permettra de visualiser le chemin emprunté par une politique gloutonne par rapport à une fonction action-valeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b4d8e-ed7b-4ceb-aac6-185945a016e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_path(q):\n",
    "    cmap = pl.cm.Reds\n",
    "    my_cmap = cmap(np.arange(cmap.N))\n",
    "    my_cmap[:,-1] = np.linspace(0, 1, cmap.N)\n",
    "    my_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "    array = np.zeros_like(q[:,:,0])\n",
    "\n",
    "    env_ = env()\n",
    "    s = (0,0)\n",
    "    prev_states = []\n",
    "\n",
    "    while s not in prev_states:\n",
    "        prev_states.append(s)\n",
    "        array[s] = 1\n",
    "        a = np.argmax(q[s])\n",
    "        _, s = env_.take_action(a)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(q.shape[1]+1, q.shape[0]+1))\n",
    "    ax.set_xticks(np.arange(0, q.shape[1]+1, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(0, q.shape[0]+1, 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color='black', linestyle='-', linewidth=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    im = ax.imshow(array, cmap=my_cmap, interpolation='nearest', extent=[0, q.shape[1], 0, q.shape[0]], alpha=.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f819d8-34ef-47f1-8588-cd582eb66c87",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Une fonction action-valeur sera représentée par un array de taille 4*12*4, où les deux premières dimensions correspondent à l'état et la dernière à l'action. \n",
    "\n",
    "*Question 1*: Écrire une fonction `draw_action_greedy_policy` qui renvoie l'action choisie (aléatoirement) par une politique $\\varepsilon$-gloutonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf30bd5c-b4a4-42e2-b1df-2df5421f28ad",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def draw_action_greedy_policy(s, q, eps=0):\n",
    "    # On pourra utiliser les fonction np.where(), np.random.binomial(), np.random.randint()\n",
    "\n",
    "    # Compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101d4ce-133c-4438-be4a-d393f838b522",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Pour implémenter les méthodes d'apprentissage par renforcement, il sera pratique de représenter les fonctions action-valeur de la façon suivante: d'une part pour chaque couple état-action, la somme des valeurs calculées pour les mises à jour (et non la moyenne), et d'autre part le nombre de mises à jour, de sorte que la fonction action-valeur qui est l'approximation de $q_*$ peut être déduite en divisant chaque composante par le nombre de mises à jour correspondant. La fonction suivante effectue ce calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c466c-79eb-4ac4-b952-d7873f257e76",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def q_mean(q_cumul, n_updates):\n",
    "    return np.divide(q_cumul, n_updates, out=np.zeros_like(q_cumul), where=(n_updates != 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ab56a5-86d0-4e31-b5e3-8e2e253ce2fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Question 2*: Implémenter le Q-learning, où étant donné un état $s$, l'action $a$ est choisie par une politique $\\varepsilon$-gloutonne par rapport à la fonction état-valeur courante. Essayer différentes valeurs de $\\varepsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d08038-aebf-4817-9cb7-81f32471f88e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = .5\n",
    "n_iter = 100000\n",
    "environment = env()\n",
    "s = (0,0)\n",
    "q_cumul = np.zeros((env.height, env.width, 4))\n",
    "n_updates = np.zeros((env.height, env.width, 4), dtype=int)\n",
    "for k in range(n_iter):\n",
    "    # compléter\n",
    "plot_path(q_mean(q_cumul, n_updates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eef7c8-b2c9-43c4-8862-12389a4fa289",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "*Question 3*: Implémenter SARSA. Comparer les résultats obtenus avec ceux issus du Q-learning. Quel est ici l'effet de la valeur de $\\varepsilon$ ? Essayer également en faisant tendre $\\varepsilon$ vers 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/home/joon/anaconda3/bin/python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": false
   },
   "name": "python3"
  },
  "name": "5-TP.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
